
# AlignÂ³GR: Unified Multi-Level Alignment for LLM-based Generative Recommendation

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“– Project Overview

AlignÂ³GR is an innovative framework designed to address semantic and behavioral misalignment challenges when applying Large Language Models (LLMs) to recommender systems. Our approach bridges this gap by unifying alignment across three levels:

- **Token-level Alignment**: Dual tokenization fusing user-item semantic and collaborative signals
- **Behavior Modeling-level Alignment**: Enhanced behavior modeling with bidirectional semantic alignment
- **Preference-level Alignment**: Progressive DPO strategy combining self-play (SP-DPO) and real-world feedback (RF-DPO)

### ğŸ¯ Key Features

- **Multi-level Alignment**: Unified alignment across token, behavior, and preference levels
- **Progressive Training**: Innovative DPO strategy combining self-play and real-world feedback
- **Efficient Training**: Support for LoRA fine-tuning and DeepSpeed distributed training
- **Modular Design**: Clear module separation for easy extension and maintenance

## ğŸ—ï¸ Project Structure

```
Align3GR/
â”œâ”€â”€ README.md                    # Project documentation
â”œâ”€â”€ data/                        # Data files
â”‚   â”œâ”€â”€ Instruments/           # Instruments dataset
â”œâ”€â”€ SFT/                        # Supervised Fine-Tuning module
â”‚   â”œâ”€â”€ lora_finetune.py       # LoRA fine-tuning implementation
â”‚   â”œâ”€â”€ mydata.py              # Data processing
â”‚   â”œâ”€â”€ my_prompt.py           # Prompt templates
â”‚   â”œâ”€â”€ collator.py            # Data collator
â”‚   â”œâ”€â”€ evaluate.py            # Evaluation script
â”‚   â”œâ”€â”€ generate_output.py     # Output generation
â”‚   â”œâ”€â”€ utils.py               # Utility functions
â”‚   â”œâ”€â”€ run_train.sh           # Training script
â”‚   â””â”€â”€ generate.sh            # Generation script
â”œâ”€â”€ RL/                         # Reinforcement Learning module
â”‚   â”œâ”€â”€ softmax_dpo.py         # DPO training implementation
â”‚   â”œâ”€â”€ PNM.py                 # Progressive negative sample generation
â”‚   â”œâ”€â”€ SM.py                  # Sentiment negative sample matching
â”‚   â”œâ”€â”€ data4RF.py            # RL data preparation
â”‚   â”œâ”€â”€ inference.py           # Inference script
â”‚   â”œâ”€â”€ evaluate_batch.py      # Batch evaluation
â”‚   â”œâ”€â”€ run_RF-dpo_training.sh # RF-DPO training script
â”‚   â”œâ”€â”€ run_SP-dpo_training.sh # SP-DPO training script
â”‚   â”œâ”€â”€ inference.sh           # Inference script
â”‚   â””â”€â”€ trainer/               # Trainer module
â”œâ”€â”€ Token/                      # Token Alignment module
â”‚   â”œâ”€â”€ main.py               # Main training script
â”‚   â”œâ”€â”€ trainer.py            # Trainer implementation
â”‚   â”œâ”€â”€ datasets.py           # Dataset classes
â”‚   â”œâ”€â”€ generate_indices.py   # Index generation
â”‚   â”œâ”€â”€ tokenize.sh           # Tokenization script
â”‚   â”œâ”€â”€ train_tokenizer.sh    # Tokenizer training
â”‚   â””â”€â”€ models/               # Model definitions
â””â”€â”€ config/                   # Configuration files
    â”œâ”€â”€ ds_z2_bf16.json      # DeepSpeed configurations
    â”œâ”€â”€ ds_z2_fp16.json
    â”œâ”€â”€ ds_z3_bf16.json
    â”œâ”€â”€ ds_z3_fp16.json
    â”œâ”€â”€ ds_z3_bf16_save16bit.json
    â””â”€â”€ ds_z3_fp16_save16bit.json
```



## ğŸ“š Usage Guide

### 1. Token Level Alignment (Tokenization)

```bash
bash Token/get_emb/process4cf.sh
bash Token/get_emb/train_din.sh

python Token/get_emb/semantic_embedding_item.py --plm_name t5 --plm_checkpoint_t5 sentence-transformers/sentence-t5-base --gpu_id 0
python Token/get_emb/semantic_embedding_user.py --plm_name t5 --plm_checkpoint_t5 sentence-transformers/sentence-t5-base --gpu_id 0

bash Token/get_emb/getting_user_cf.sh

bash Token/train_tokenizer.sh

bash Token/tokenize.sh
```

### 2. Behaviors Modeling Level Alignment (Multi-task SFT)

```bash
bash SFT/run_train.sh

bash SFT/generate_output.sh
```

### 3. Preference Level Alignment (Preference-based RL)

```bash

python RL/PNM.py 

bash RL/run_SP-dpo_training.sh

bash RL/inference_SP.sh

bash RL/data4RF.sh

python RL/SM.py

bash RL/run_RF-dpo_training.sh

bash RL/inference_RF.sh

```


## âš™ï¸ Configuration

The project provides various DeepSpeed configuration options:

- `ds_z2_*.json`: ZeRO-2 configuration for medium-scale training
- `ds_z3_*.json`: ZeRO-3 configuration for large-scale training
- `*_fp16.json`: FP16 precision for memory efficiency
- `*_bf16.json`: BF16 precision for better numerical stability

# Align3GR
